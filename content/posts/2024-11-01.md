+++
date = '2024-11-01'
draft = false
title = 'Thinking and Learning Like People'
author = ['Julian', 'Tasha']
+++
[***Building machines that learn and think with people.***](https://www.nature.com/articles/s41562-024-01991-9) Collins, K.M., Sucholutsky, I., Bhatt, U. et al.

<!--more-->
---

This position piece argues that good AI thought partners are ones which are interpretable to humans. Specifically, it points to several exemplar AI systems based on Bayesian inference. These models are thought to be more interpretable since they represent beliefs explicitly, using explicit probabilities over explicit predicates.

These models certainly are more interpretable at face value! In comparison, when looking at a big, deep neural network, it's almost impossible to determine the role of a particular neuron. However, this led to some interesting questions in our discussion:

**Why should an AI thought partner be interpretable?**

The best thought partners we have today are other humans. If deep neural networks are hard to interpret, the human brain is even more so.

Yet, we don't find ourselves wishing that other humans were more simpler, more interpretable. (well, maybe some of us do!) In fact, this complexity is something we really enjoy about thinking with other humans: we don't know exactly how each others' minds work. Other people's minds are full of surprises, bringing completely new ideas to the table thanks to their varying backgrounds and experiences.

And this soup of uninterpretable action-potentials leads to amazing results. A group of human minds thinking together is like a chaotic system with emergent behavior. We come up with great ideas together in part thanks to this uninterpretable mixture of behaviors; the outcome of the whole is greater than the sum of its parts.

This raises the question of whether AI systems need be interpretable. Though, While we probably can't interpret each others' brains at the level of action-potentials, humans do have a way of eventually understanding each other at higher levels of abstraction. And maybe this is really the kind of interpretability we are after: the ability to use communication to figure out what an AI system believes. But it remains an open question whether beliefs should be represented symbolically or allowed to be emergent properties of deep connectionist systems.

**What does it mean for an AI system to be interpretable?**

Deep neural networks are notorious for being black-boxy. But this could be due to a lack of good tools.

Maybe instead of trying to understand neurons as doing a particular task, we can explore the possibility of neurons taking part in a _superposition_ of tasks, like in this paper from [Anthropic](https://transformer-circuits.pub/2022/toy_model/index.html).

What other techniques can we borrow from modern interpretability research in order to try to understand deep networks, both biological and artificial?

**What would make a great AI thought partner?**

What would be the interface? What modalities would it support? If embodiment is important, what's the ideal form of interaction? And what kinds of problems could we solve with such a partner?
